# SAIGEN Configuration Sample
# This file contains all available configuration options with their default values
# Copy this file to one of the following locations and customize as needed:
#   - ~/.saigen/config.yaml (recommended)
#   - ~/.saigen/config.json
#   - ./.saigen.yaml
#   - ./.saigen.json
#   - ./saigen.yaml
#   - ./saigen.json

# Core Configuration
config_version: "0.1.0"
log_level: "info"  # debug, info, warning, error
log_file: null  # Optional path to log file, e.g., "/path/to/saigen.log"

# LLM Provider Configurations
# Configure one or more LLM providers for AI-enhanced metadata generation
llm_providers:
  openai:
    provider: "openai"
    api_key: null  # Set via OPENAI_API_KEY environment variable or here
    api_base: null  # Optional custom API base URL
    model: "gpt-4o-mini"
    max_tokens: 4000
    temperature: 0.1
    timeout: 30
    max_retries: 3
    enabled: true
  
  # Example Anthropic configuration (uncomment and configure to use)
  # anthropic:
  #   provider: "anthropic"
  #   api_key: null  # Set via ANTHROPIC_API_KEY environment variable
  #   api_base: null
  #   model: "claude-3-haiku-20240307"
  #   max_tokens: 4000
  #   temperature: 0.1
  #   timeout: 30
  #   max_retries: 3
  #   enabled: true

  # Example Ollama local inference configuration (uncomment and configure to use)
  # Requires Ollama to be installed and running locally
  # ollama:
  #   provider: "ollama"
  #   base_url: "http://localhost:11434"  # Default Ollama server URL
  #   model: "llama2"  # Model name (must be installed in Ollama)
  #   temperature: 0.1
  #   timeout: 60  # Longer timeout for local models
  #   max_retries: 3
  #   enabled: true
  #   # Popular model options (install with: ollama pull <model>):
  #   # - "llama2" (7B, good balance of speed and quality)
  #   # - "llama2:13b" (13B, better quality, slower)
  #   # - "codellama" (specialized for code generation)
  #   # - "mistral" (7B, fast and efficient)
  #   # - "mixtral" (8x7B mixture of experts)
  #   # - "phi" (3B, very fast, good for simple tasks)

  # Example OpenAI-compatible local inference configuration
  # Works with LM Studio, LocalAI, vLLM, and other OpenAI-compatible servers
  # openai:
  #   provider: "openai"
  #   api_key: "not-needed"  # Most local servers don't require API keys
  #   api_base: "http://localhost:1234/v1"  # LM Studio default URL
  #   model: "local-model"  # Model name as configured in your local server
  #   max_tokens: 4000
  #   temperature: 0.1
  #   timeout: 60  # Longer timeout for local models
  #   max_retries: 3
  #   enabled: true
  #   # Common local server URLs:
  #   # - LM Studio: "http://localhost:1234/v1"
  #   # - LocalAI: "http://localhost:8080/v1"
  #   # - vLLM: "http://localhost:8000/v1"
  #   # - Oobabooga text-generation-webui: "http://localhost:5000/v1"

# Repository Configurations
# Configure package repositories for metadata collection
repositories: {}
  # Example repository configurations:
  # ubuntu_main:
  #   name: "ubuntu_main"
  #   type: "apt"
  #   url: "http://archive.ubuntu.com/ubuntu"
  #   enabled: true
  #   cache_ttl: 3600
  #   priority: 1
  #   credentials: null
  # 
  # homebrew:
  #   name: "homebrew"
  #   type: "brew"
  #   url: null
  #   enabled: true
  #   cache_ttl: 3600
  #   priority: 1
  #   credentials: null

# Cache Configuration
cache:
  directory: "~/.saigen/cache"  # Cache directory path
  max_size_mb: 1000  # Maximum cache size in megabytes
  default_ttl: 3600  # Default cache TTL in seconds (1 hour)
  cleanup_interval: 86400  # Cache cleanup interval in seconds (24 hours)

# RAG (Retrieval-Augmented Generation) Configuration
rag:
  enabled: true
  index_directory: "~/.saigen/rag_index"  # RAG index storage directory
  embedding_model: "sentence-transformers/all-MiniLM-L6-v2"
  max_context_items: 5  # Maximum number of context items to include
  similarity_threshold: 0.7  # Minimum similarity threshold for context matching
  rebuild_on_startup: false  # Whether to rebuild index on startup
  
  # Sample data configuration for LLM examples
  default_samples_directory: null  # Auto-detected or set via config samples command
  use_default_samples: true  # Whether to use sample data in LLM prompts
  max_sample_examples: 3  # Maximum number of sample examples to include

# Validation Configuration
validation:
  schema_path: null  # Optional custom schema path (uses built-in schema if null)
  strict_mode: true  # Enable strict validation mode
  auto_fix_common_issues: true  # Automatically fix common validation issues
  validate_repository_accuracy: true  # Validate against repository data

# Generation Configuration
generation:
  default_providers:  # Default providers to generate metadata for
    - "apt"
    - "brew"
    - "winget"
  output_directory: "./saidata"  # Output directory for generated files
  backup_existing: true  # Backup existing files before overwriting
  parallel_requests: 3  # Number of parallel requests to repositories
  request_timeout: 120  # Request timeout in seconds

# Advanced Settings
user_agent: "saigen/0.1.0"  # User agent string for HTTP requests
max_concurrent_requests: 5  # Maximum concurrent requests across all operations
request_timeout: 30  # Default request timeout in seconds

# Environment Variable Overrides
# The following environment variables can override configuration values:
# - OPENAI_API_KEY: Sets llm_providers.openai.api_key
# - ANTHROPIC_API_KEY: Sets llm_providers.anthropic.api_key
# - SAIGEN_LOG_LEVEL: Sets log_level
# - SAIGEN_CACHE_DIR: Sets cache.directory
# - SAIGEN_OUTPUT_DIR: Sets generation.output_directory

# Local Inference Setup Notes:
# 1. Ollama: Install from https://ollama.ai and run 'ollama pull <model>' to download models
# 2. LM Studio: Download from https://lmstudio.ai and load a model in the server tab
# 3. LocalAI: Run with Docker or install locally, see https://localai.io
# 4. vLLM: Install with 'pip install vllm' and run with 'python -m vllm.entrypoints.openai.api_server'