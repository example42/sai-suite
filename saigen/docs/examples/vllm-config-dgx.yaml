# SAIGEN Configuration for vLLM on NVIDIA GB10 (Grace Blackwell) Systems
# Supports both local (on GB10) and remote (development machine) setups

config_version: "0.1.0"
log_level: "info"

# ============================================================================
# OPTION 1: Local Setup (SAIGEN running on GB10)
# ============================================================================
# llm_providers:
#   vllm_primary:
#     provider: "vllm"
#     base_url: "http://localhost:8000/v1"
#     model: "meta-llama/Meta-Llama-3-8B-Instruct"
#     temperature: 0.1
#     max_tokens: 4096
#     timeout: 90
#     enabled: true
#     priority: "high"

# ============================================================================
# OPTION 2: Remote Setup (SAIGEN on development machine, vLLM on GB10)
# ============================================================================
llm_providers:
  vllm_primary:
    provider: "vllm"
    base_url: "http://gb10-hostname:8000/v1"  # Replace with GB10 IP/hostname
    # Examples:
    # base_url: "http://192.168.1.100:8000/v1"  # GB10 IP address
    # base_url: "http://gb10.local:8000/v1"     # mDNS hostname
    model: "meta-llama/Meta-Llama-3-8B-Instruct"
    temperature: 0.1
    max_tokens: 4096
    timeout: 120  # Longer timeout for network latency
    enabled: true
    priority: "high"
    # Server configuration (set when starting vLLM server on GB10):
    # --host 0.0.0.0 (required for remote access)
    # --gpu-memory-utilization 0.90
    # --max-model-len 8192
  
  # Alternative: Larger model on GB10 (leveraging 128GB unified memory)
  vllm_mixtral:
    provider: "vllm"
    base_url: "http://gb10-hostname:8001/v1"
    model: "mistralai/Mixtral-8x7B-Instruct-v0.1"
    temperature: 0.1
    max_tokens: 4096
    timeout: 120
    enabled: false  # Enable if running second vLLM instance
    priority: "medium"
  
  # Optional: Cloud fallback for when local is unavailable
  openai_fallback:
    provider: "openai"
    api_key: null  # Set via OPENAI_API_KEY
    model: "gpt-4o-mini"
    max_tokens: 4000
    temperature: 0.1
    timeout: 30
    enabled: false  # Enable if you want cloud fallback
    priority: "low"

# Repository Configurations
repositories:
  ubuntu_main:
    name: "ubuntu_main"
    type: "apt"
    url: "http://archive.ubuntu.com/ubuntu"
    enabled: true
    cache_ttl: 3600
    priority: 1
  
  homebrew:
    name: "homebrew"
    type: "brew"
    enabled: true
    cache_ttl: 3600
    priority: 1

# Cache Configuration (optimized for DGX)
cache:
  directory: "~/.saigen/cache"
  max_size_mb: 10000  # 10GB cache for large-scale generation
  default_ttl: 3600
  cleanup_interval: 86400

# RAG Configuration
rag:
  enabled: true
  index_directory: "~/.saigen/rag_index"
  embedding_model: "sentence-transformers/all-MiniLM-L6-v2"
  max_context_items: 5
  similarity_threshold: 0.7
  rebuild_on_startup: false
  use_default_samples: true
  max_sample_examples: 3

# Validation Configuration
validation:
  schema_path: null
  strict_mode: true
  auto_fix_common_issues: true
  validate_repository_accuracy: true

# Generation Configuration
generation:
  default_providers:
    - "apt"
    - "brew"
    - "winget"
  output_directory: "./saidata"
  backup_existing: true
  # Local setup (SAIGEN on GB10):
  parallel_requests: 15  # Higher for local access
  # Remote setup (SAIGEN on dev machine):
  # parallel_requests: 10  # Moderate for network latency
  request_timeout: 90
  
  # URL Validation
  enable_url_filter: true
  url_filter_timeout: 5
  url_filter_max_concurrent: 15

# Advanced Settings
user_agent: "saigen/0.1.0"
max_concurrent_requests: 10
request_timeout: 90

# ============================================================================
# GB10 Setup Instructions
# ============================================================================
#
# Hardware Specs:
# - GPU: NVIDIA Blackwell with 5th Gen Tensor Cores (1 PFLOP FP4)
# - CPU: 20-core Arm (10x Cortex-X925 + 10x Cortex-A725)
# - Memory: 128 GB LPDDR5x unified coherent @ 273 GB/s
# - Network: 10 GbE + ConnectX-7 @ 200 Gbps + WiFi 7
# - Storage: 4 TB NVMe M.2
# - Power: 240W TDP
#
# 1. On GB10: Start vLLM server
#    python -m vllm.entrypoints.openai.api_server \
#        --model meta-llama/Meta-Llama-3-8B-Instruct \
#        --gpu-memory-utilization 0.90 \
#        --max-model-len 8192 \
#        --host 0.0.0.0 \
#        --port 8000
#
# 2. GB10's 128GB unified memory allows larger models:
#    - Llama 3 8B: ~16GB (recommended, fast)
#    - Mixtral 8x7B: ~90GB (high quality, leverages unified memory)
#    - CodeLlama 34B: ~70GB (code generation)
#    - Llama 3 70B (AWQ): ~40GB (best quality with quantization)
#
# 3. Network Setup:
#    - Local (SAIGEN on GB10): Use localhost, higher concurrency
#    - Remote (SAIGEN on dev machine): Use GB10 IP, moderate concurrency
#    - 10 GbE: Good for moderate batches
#    - ConnectX-7 @ 200 Gbps: Excellent for large batches
#    - WiFi 7: Convenient but slower
#
# 4. Monitor GPU usage: nvidia-smi -l 1
#
# 5. Batch generation examples:
#    # Local on GB10:
#    saigen batch generate list.txt --llm-provider vllm_primary --max-concurrent 15
#    
#    # Remote over 200 Gbps network:
#    saigen batch generate list.txt --llm-provider vllm_primary --max-concurrent 20
#    
#    # Remote over WiFi 7:
#    saigen batch generate list.txt --llm-provider vllm_primary --max-concurrent 5
